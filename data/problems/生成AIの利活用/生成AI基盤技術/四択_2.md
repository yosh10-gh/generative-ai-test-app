## 四択_2
### 生成AI基盤技術
生成AIの性能評価指標として最も適切な組み合わせは何ですか？

A. Perplexityのみを使用した単一指標評価
B. 人間評価のみに依存した主観的評価
C. BLEU、ROUGE、BERTScore、人間評価を組み合わせた多面的評価
D. 処理速度とメモリ使用量のみを重視した効率性評価

答え：C

解説：
生成AIの性能評価では、「BLEU、ROUGE、BERTScore、人間評価を組み合わせた多面的評価」が最も適切です。

**多面的評価の必要性**：
- 単一指標では生成AIの複雑な能力を適切に評価できない
- 異なる指標が異なる側面（流暢性、正確性、創造性等）を測定
- 実用性と技術的性能の両方を考慮する必要

**2024-2025年の評価指標進化**：
- **LLM-as-a-Judge**：GPT-4等を評価者として活用する手法の普及
- **Zero-shot Benchmarking**：事前学習なしでの評価能力測定
- **Faithfulness評価**：生成内容の事実正確性を重視
- **Toxicity/Harmfulness評価**：安全性と倫理性の評価強化
- **SelfCheckGPT**：外部知識なしでハルシネーション検出
- **FActScore**：原子的事実の正確性評価手法

**各評価指標の役割**：
- **BLEU/ROUGE**：参照文との類似度、基本的な品質指標
- **BERTScore**：意味的類似度、文脈理解の評価
- **人間評価**：実用性、満足度、創造性の評価
- **新興指標**：Distinct-N（多様性）、SELF-BLEU（一貫性）

**最新研究の知見**：
- **TruthfulQA研究**：事実正確性評価の標準化
- **HaluEval研究**：包括的ハルシネーション評価フレームワーク
- **MT評価研究**：生成ベース評価がスコアベース評価より優秀
- **Constitutional AI研究**：自己修正による品質向上

**実践的な評価フレームワーク**：
- **自動評価**：効率性と一貫性を提供
- **人間評価**：実用性と満足度を測定
- **ハイブリッド評価**：両者の利点を組み合わせ
- **継続的評価**：運用中の性能監視

**他の選択肢の問題点**：
- A：Perplexityは言語モデルの基本性能のみ、実用性は測定不可
- B：人間評価のみでは主観性とコストの問題
- D：効率性は重要だが、出力品質を無視すると実用性が低下

**評価における注意点**：
- 評価指標間の相関と補完関係の理解
- タスク特性に応じた指標の重み付け
- 評価コストと精度のバランス
- 継続的な評価基準の見直し

現代の生成AI評価では、技術的指標と人間中心の評価を統合した包括的アプローチが標準となっています。 