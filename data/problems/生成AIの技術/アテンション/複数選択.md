## 複数選択
### アテンション
深層学習で使用されるアテンション機構の種類として正しいものを全て選びなさい。

A. Self-Attention（自己注意機構）
B. Cross-Attention（交差注意機構）
C. Local Attention（局所注意機構）
D. Multi-Head Attention（マルチヘッド注意機構）

答え：A、B、C、D

解説：
選択肢A：Self-Attention（自己注意機構）は正解です。入力シーケンス内の各要素が同じシーケンスの他の全ての要素との関係を計算する機構です。Transformerの中核技術で、Q、K、Vが全て同じ入力から生成されます。「私は昨日映画を見た」という文では、「私」が「昨日」「映画」「見た」との関連性を同時に計算し、文脈理解を深めます。BERT、GPTなどの大規模言語モデルで広く使用されています。

選択肢B：Cross-Attention（交差注意機構）は正解です。異なる入力シーケンス間の関係を計算する機構で、Encoder-Decoder architectureで使用されます。機械翻訳では、デコーダ（日本語出力）がエンコーダ（英語入力）の情報に注意を向ける際に使用されます。QはDecoderから、K、VはEncoderから生成され、翻訳品質の向上に重要な役割を果たします。

選択肢C：Local Attention（局所注意機構）は正解です。計算効率を考慮して、全シーケンスではなく局所的な範囲内でのみアテンションを計算する機構です。ウィンドウサイズを制限することで、O(n²)の計算複雑度を削減できます。長文処理や計算資源が限られた環境で特に有効で、Longformerなどのモデルで実装されています。

選択肢D：Multi-Head Attention（マルチヘッド注意機構）は正解です。複数のアテンションヘッドを並列で動作させ、異なる表現部分空間での関係性を同時に学習する機構です。各ヘッドが異なる言語的側面（構文、意味、位置関係など）に注目でき、最終的に全ヘッドの出力を結合して豊かな表現を生成します。TransformerにおけるMultiHead(Q,K,V) = Concat(head₁,...,headₕ)W^Oの式で表現され、現代の大規模言語モデルの標準アーキテクチャとなっています。 