## 四択_③
### アテンション
Scaled Dot-Product Attention において、アテンションスコアを√d_k で除算する理由として最も適切なものはどれか。

A. 計算速度を向上させるため
B. メモリ使用量を削減するため
C. 勾配の安定性を保つため
D. 過学習を防止するため

答え：C

解説：
Scaled Dot-Product Attentionでアテンションスコア（QK^T）を√d_k で除算する主な理由は、選択肢Cの「勾配の安定性を保つため」です。内積QK^Tの値は、キーの次元d_kが大きくなるにつれて大きくなる傾向があります。これは、d_k次元のランダムベクトルの内積の分散がd_kに比例するためです。大きなスコア値がsoftmax関数に入力されると、softmax出力が極端に偏った分布になり（例：[0.99, 0.01, 0.0, ...]）、勾配が非常に小さくなって学習が停滞する問題が発生します。√d_k による正規化は、内積の分散をほぼ一定に保ち、softmax関数が適度な勾配を持つ領域で動作するように調整します。これにより、安定した学習が可能になります。選択肢Aの計算速度向上は除算により実際は低下し、選択肢Bのメモリ削減効果はなく、選択肢Dの過学習防止は直接的な目的ではありません。この√d_k スケーリングは「Attention is All You Need」論文で導入され、現在のTransformerアーキテクチャの標準となっています。 