## 複数選択
### オープン大規模言語モデル
2024-2025年のオープン大規模言語モデルの重要な技術的特徴として適切なものを全て選択してください。

A. MoE（Mixture of Experts）アーキテクチャによる効率的なスケーリング
B. マルチモーダル機能（テキスト・画像・音声の統合処理）
C. 完全にクローズドソースでの開発・運用
D. 長文コンテキスト処理能力（128K〜10Mトークン）
E. 特定企業による独占的なAPI提供のみ

答え：A、B、D

解説：
2024-2025年のオープン大規模言語モデルの重要な技術的特徴として、以下の要素が挙げられます：

**A. MoE（Mixture of Experts）アーキテクチャによる効率的なスケーリング**：
Mixtral 8x22B（総1410億、活性化390億）、DeepSeek-V3（総671B、活性化37B）、Qwen 3（235B-A22B）、Llama 4 Maverick（170億活性化、128エキスパート）など、多くの最新オープンモデルがMoEアーキテクチャを採用し、計算効率と性能を両立しています。

**B. マルチモーダル機能（テキスト・画像・音声の統合処理）**：
Llama 4（ネイティブマルチモーダル）、Qwen2-VL-72B（視覚言語モデル）、QVQ-72B-Preview（多模態推理）、Qwen2-Audio（音声処理）、PaliGemma 2（視覚言語）、Pixtral Large（124B多模態）など、テキスト以外のモダリティを統合的に処理する能力が標準化されています。

**D. 長文コンテキスト処理能力（128K〜10Mトークン）**：
Llama 4 Scout（最大10Mトークン）、Llama 3.1 405B（128K）、Gemma 3（128K）、Qwen 2.5（128K）、Mistral Medium 3（128K）、DeepSeek-V3（128K）など、大幅に拡張されたコンテキストウィンドウにより、長大な文書や対話の処理が可能になっています。

**不適切な選択肢：**
- C：オープンモデルは定義上、重みやアーキテクチャが公開されており、クローズドソースではありません
- E：オープンモデルは、Hugging Face、GitHub、Ollama、vLLMなど多様なプラットフォームで利用可能で、特定企業の独占ではありません

これらの技術革新により、2024-2025年は「オープンLLMの年」と呼ばれ、プロプライエタリモデルに匹敵する性能を達成しています。 