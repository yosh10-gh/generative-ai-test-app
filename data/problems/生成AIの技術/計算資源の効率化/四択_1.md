## 四択_1
### 計算資源の効率化
LoRA（Low-Rank Adaptation）に関する説明として最も適切なものはどれですか？

A. 事前学習済みモデルの重みを固定し、低ランク行列の分解を用いて少数のパラメータのみを学習する手法
B. モデルの全パラメータを量子化して計算精度を下げることで高速化を図る手法
C. 複数の専門家モデルを組み合わせて必要な部分のみを活性化する手法
D. モデルの不要な結合を除去してネットワーク構造を簡素化する手法

答え：A

解説：
LoRA（Low-Rank Adaptation）は、事前学習済みの大規模言語モデルを効率的にファインチューニングするための手法です。元のモデルの重みを固定したまま、低ランク行列の分解（A×B）を用いて追加の学習可能パラメータを導入します。これにより、全パラメータの1%未満の学習で、フルファインチューニングに匹敵する性能を実現できます。選択肢Bは量子化、Cは Mixture of Experts（MoE）、Dはプルーニングの説明です。 