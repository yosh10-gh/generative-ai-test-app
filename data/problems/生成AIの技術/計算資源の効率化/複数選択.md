## 問題
### 計算資源の効率化
計算資源の効率化技術に関する説明として正しいものをすべて選んでください。

A. プルーニング（枝刈り）は、モデルの重要度の低い結合や重みを除去することでモデルサイズと計算量を削減する

B. 知識蒸留は、大規模な教師モデルの知識を小規模な生徒モデルに転移することで効率化を図る手法である

C. グラディエント・アキュムレーションは、メモリ使用量を削減するために勾配の精度を下げる手法である

D. チェックポインティングは、中間的な活性化値を再計算することでメモリ使用量を時間と引き換えに削減する手法である

## 正解A、B、D

解説：
各選択肢について正誤を個別に説明

**選択肢A（○）：プルーニング**
プルーニングは、モデルの性能に与える影響が小さい重みや結合を特定し、それらを除去することでモデルの計算量とメモリ使用量を削減する代表的な効率化手法です。
- **構造化プルーニング**: チャネル、フィルタ、層単位での除去
- **非構造化プルーニング**: 個別の重み単位での除去
- **最新動向**: 2024-2025年では、LLMに特化したプルーニング手法が活発に研究されています

**選択肢B（○）：知識蒸留**
知識蒸留は、大規模で高性能な教師モデル（Teacher Model）の出力分布や中間表現を、より小規模な生徒モデル（Student Model）に学習させる手法です。
- **出力蒸留**: 教師モデルの最終出力（ソフトラベル）を学習
- **特徴蒸留**: 中間層の特徴表現を学習
- **実用例**: ChatGPT → GPT-3.5-turbo、Claude → Claude Instant等

**選択肢C（×）：グラディエント・アキュムレーション**
グラディエント・アキュムレーションは、勾配の精度を下げる手法ではありません。小さなバッチサイズで複数回の順伝播・逆伝播を行い、勾配を蓄積してから一度にパラメータを更新する手法です。
- **目的**: メモリ制約下で大きな実効バッチサイズを実現
- **仕組み**: gradient = Σ(mini_batch_gradients) / accumulation_steps
- **効果**: メモリ使用量を抑えながら大バッチサイズの効果を得る

**選択肢D（○）：チェックポインティング**
チェックポインティング（Gradient Checkpointing）は、順伝播時に一部の中間活性化値を保存せず、逆伝播時に必要に応じて再計算する手法です。
- **メモリ削減**: 活性化値の保存量を大幅削減（50-80%削減可能）
- **計算コスト**: 再計算により約33%の計算時間増加
- **実装**: PyTorchのtorch.utils.checkpoint、Transformersのgradient_checkpointing等 