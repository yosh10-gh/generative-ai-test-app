## 問題
### 計算資源の効率化
Mixture of Experts（MoE）アーキテクチャの実用的な利点として最も適切なものはどれですか？

A. モデル全体のパラメータ数を物理的に削減することで、ストレージ容量を節約できる

B. 全ての専門家モデルを同時に活性化することで、並列処理性能が向上する

C. 学習時に必要なデータセットのサイズを大幅に削減できる

D. 推論時に必要な専門家のみを活性化することで、大規模モデルでも効率的な計算が可能

## 正解D

解説：
Mixture of Experts（MoE）の最大の利点は、スパース化による計算効率の向上です。推論時には全ての専門家モデルではなく、入力に最も適した少数の専門家のみを選択的に活性化します。

**実際の例**：
- **GPT-4**: 推定1.8兆パラメータを持ちながら、推論時には約2200億パラメータ相当の専門家のみが動作
- **DeepSeek-V3**: 6710億パラメータのうち、推論時には約370億パラメータのみが活性化
- **Llama 4**: MoEアーキテクチャにより、170億アクティブパラメータで効率的な推論を実現

**MoEの技術的メリット**：
1. **計算効率**: 必要な専門家のみを活性化することで計算コストを大幅削減
2. **スケーラビリティ**: 総パラメータ数を増やしながら推論コストを抑制
3. **専門化**: 各専門家が特定のタスクやドメインに特化して学習

これにより、大規模なモデル容量を維持しながら計算コストを大幅に削減できます。選択肢Aはパラメータ数は実際には増加し、選択肢Bは全専門家の同時活性化は非効率、選択肢Cはデータセット削減とは無関係です。2024-2025年の最新研究では、MoEは大規模言語モデルの標準的なアーキテクチャとして広く採用されています。 