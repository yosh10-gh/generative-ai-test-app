# データセットのサイズ - 四択問題2

## 問題
合成データ生成における「SynthLLM」フレームワークの技術的特徴と、データセットサイズのスケーリング法則について、最も正確な記述はどれか。

A. SynthLLMは単純なデータ複製により量を増やすだけで、品質向上は期待できず、従来のべき乗則（y = ax^(-b)）に完全に従う

B. 合成データは300Bトークンまでは線形的に性能が向上し、それ以降は急激に性能が低下するため、実用性は限定的である

C. SynthLLMは文書間の高レベル概念をグラフアルゴリズムで抽出・再結合し、修正スケーリング法則（L(D) = B/(D_l + D^β) + E）に従って予測可能な性能向上を実現する

D. データ圧縮率とスケーリング特性は無関係であり、gzipによる圧縮性はモデル性能の予測には使用できない

## 正解
C

## 解説
合成データ生成とスケーリング法則について、2024年の最新研究成果を正確に理解する必要があります。

**正解（C）の技術的根拠**：

**SynthLLMの革新的手法**：
- **Level-1**: 単一文書からの直接抽出
- **Level-2**: 文書内の概念分解・再結合による多様性向上
- **Level-3**: 複数文書間のグローバル概念グラフ構築
- **グラフアルゴリズム**: 共起統計に基づく重み付きエッジ（w(u,v) = log(freq(u,v) + ε)）

**修正スケーリング法則の数学的定式化**：
```
L(D) = B/(D_l + D^β) + E
```
- **D_l**: 事前学習で獲得済みの潜在データサイズ
- **D**: 合成データのトークン数
- **β**: 減衰指数（モデルサイズにより異なる）
- **E**: 到達可能な最適性能の下限

**実証結果**：
- **3Bモデル**: β=0.34、4Tトークンで最適化
- **8Bモデル**: β値がより高く、1Tトークンで最適化
- **予測精度**: R²>0.98の高精度でスケーリング予測が可能

**他選択肢の問題点**：
- **A**: 単純複製ではなく、概念レベルでの知的再結合を実行
- **B**: 300B付近で改善率は鈍化するが、急激な低下はなく、モデルサイズにより最適点は変動
- **D**: gzip圧縮率は実際にデータ複雑性とスケーリング特性の有効な予測指標

**技術的意義**：
この修正スケーリング法則により、事前訓練済みモデルの知識を考慮した効率的なデータ戦略の策定が可能になり、従来の単純なべき乗則を超えた精密な性能予測が実現されています。 