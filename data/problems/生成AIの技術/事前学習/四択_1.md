# 事前学習 - 四択問題1

## 問題
深層学習における**事前学習（Pre-training）**の最も適切な定義はどれですか？

## 選択肢
A. 小規模なラベル付きデータセットで特定タスクに特化して訓練すること

B. 大規模なデータセットで汎用的な表現を学習し、後に特定タスクに適応させる訓練方法

C. データの前処理として正規化とトークン化のみを行うこと

D. 教師あり学習のみを用いて分類問題を解決すること

## 正解
**B**

## 解説
事前学習（Pre-training）は、大規模なデータセット（通常は未ラベルデータまたは自己教師ありタスク）を用いて、汎用的で有用な表現や特徴を学習する訓練方法です。

この手法の核心的な価値は以下の通りです：

**事前学習の目的**：
- 大量のデータから汎用的な表現を学習
- ドメイン知識の効率的な蓄積
- 下流タスクでの転移学習の基盤構築

**技術的特徴**：
- **教師なし学習**や**自己教師あり学習**を主に使用
- **マスク言語モデル**（BERT）や**次単語予測**（GPT）などのプレテキストタスク
- **対照学習**やオートエンコーダベースの手法

**事前学習→ファインチューニングパイプライン**：
1. **事前学習段階**：大規模データで汎用表現を学習
2. **ファインチューニング段階**：特定タスクに適応

**実例**：
- **BERT**：マスク言語モデリングでの事前学習
- **GPT系列**：自己回帰的言語モデリング
- **Vision Transformer**：画像パッチ予測
- **SimCLR**：対照学習ベースの視覚表現学習

各選択肢の解説：
- **A**: これは「ファインチューニング」の説明
- **B**: 正解。事前学習の正確な定義
- **C**: これは「前処理」の説明
- **D**: 特定の学習手法に限定した不正確な記述

事前学習は現代のAIシステム、特に**Transformer**ベースのモデルにおいて中核的な役割を果たしています。 