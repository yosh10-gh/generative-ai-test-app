# ファインチューニング - 四択問題1

## 問題
深層学習における**ファインチューニング（Fine-tuning）**の最も適切な定義はどれですか？

## 選択肢
A. 事前学習済みモデルを完全に初期化してゼロから新しいタスクを学習させること
B. 事前学習済みモデルの重みを凍結したまま、追加レイヤーのみを訓練すること
C. 事前学習済みモデルの一部または全体の重みを特定タスクに適応するように再訓練すること
D. データの前処理とハイパーパラメータの最適化のみを行うこと

## 正解
**C**

## 解説
ファインチューニング（Fine-tuning）は、既に大規模データで事前学習された汎用的なモデルを、特定のタスクやドメインに適応させるために、その重みを調整する訓練手法です。

### **ファインチューニングの核心概念**

**定義と目的**：
- **転移学習**の一種として、事前学習で獲得した知識を活用
- 特定タスクでの性能向上と計算コストの削減を同時実現
- **少ないデータ**でも高性能なモデルを構築可能

**技術的特徴**：
```python
# 典型的なファインチューニングの流れ
# 1. 事前学習済みモデルの読み込み
model = AutoModelForCausalLM.from_pretrained("base_model")

# 2. タスク固有の設定
model.config.task_specific_params = {...}

# 3. 学習率の調整（通常は事前学習より小さく）
learning_rate = 1e-5  # 事前学習: 1e-4

# 4. 段階的解凍または全重み更新
for param in model.parameters():
    param.requires_grad = True
```

### **2024-2025年の最新動向**

**Parameter-Efficient Fine-Tuning (PEFT)の台頭**：
- **LoRA (Low-Rank Adaptation)**：重み行列を低ランク分解で近似
- **QLoRA**：4bit量子化 + LoRAで大幅な省メモリ化
- **AdaLoRA**：適応的ランク配分による効率化

**技術的進歩**：
```python
# LoRAの数学的表現
W_new = W_original + α(AB^T)
# W_original: 事前学習重み（凍結）
# A, B: 学習可能な低ランク行列
# α: スケーリング係数
```

**効率化の指標**：
- **メモリ削減**：QLoRAで65B→16GB VRAM
- **計算効率**：訓練可能パラメータ0.1%以下
- **性能維持**：フル・ファインチューニングの95%以上

### **最新の手法分類**

**1. Full Fine-tuning**：
- 全重みを更新（従来手法）
- 最高性能だが計算コスト大

**2. Parameter-Efficient Fine-tuning**：
- **LoRA系列**：LoRA, QLoRA, AdaLoRA
- **Adapter系列**：Bottleneck adapters, Prefix tuning
- **Prompt系列**：P-tuning v2, Soft prompts

**3. Hybrid Approaches**：
- **レイヤー別戦略**：下位層凍結、上位層更新
- **段階的解凍**：Gradual unfreezing
- **知識蒸留併用**：Teacher-Student + Fine-tuning

### **各選択肢の解説**

**A. 完全初期化**: これは「ゼロから学習」であり、ファインチューニングではありません。事前学習の恩恵を全く受けられず、大量のデータと計算リソースが必要になります。

**B. 重み凍結 + 追加レイヤー**: これは**Feature Extraction**や**Linear Probing**と呼ばれる手法で、ファインチューニングよりも限定的なアプローチです。

**C. 重みの再訓練**: ✅ **正解** - ファインチューニングの正確な定義です。事前学習済みの重みを出発点として、特定タスクに向けて調整します。

**D. 前処理とハイパーパラメータ調整**: これは**Model Configuration**の段階であり、重みの更新を伴わないため、ファインチューニングとは異なります。

### **実装上の重要な考慮点**

**学習率設定**：
- 事前学習時より**1/10〜1/100**程度に設定
- **Layer-wise learning rate decay**の適用
- **Warm-up strategy**による安定化

**最適化戦略**：
- **AdamW**オプティマイザーが主流
- **Gradient clipping**による勾配爆発防止
- **Early stopping**による過学習回避

ファインチューニングは現代のAI開発において必須の技術であり、効率的な実装が競争優位性を決定する重要な要素となっています。 