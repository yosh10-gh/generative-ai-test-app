# ファインチューニング - 四択問題3

## 問題
**RLHF（Reinforcement Learning from Human Feedback）**と**DPO（Direct Preference Optimization）**の主な違いとして最も適切なものはどれですか？

## 選択肢
A. RLHFは教師あり学習のみを使用し、DPOは強化学習のみを使用する
B. RLHFは報酬モデルを別途訓練し強化学習で最適化、DPOは選好データから直接最適化
C. RLHFは量子化技術を必須とし、DPOは通常の精度で動作する
D. RLHFは生成タスク専用で、DPOは分類タスク専用である

## 正解
**B**

## 解説
RLHF（Reinforcement Learning from Human Feedback）とDPO（Direct Preference Optimization）は、大規模言語モデルを人間の選好に合わせる（アライメント）ための二大手法です。両者の技術的違いを理解することは、現代のLLM開発において重要です。

### **RLHF（Reinforcement Learning from Human Feedback）**

**技術的アーキテクチャ**：
```python
# RLHFの3段階プロセス
# 1. Supervised Fine-Tuning (SFT)
model_sft = train_supervised(base_model, demonstration_data)

# 2. Reward Model Training  
reward_model = train_reward_model(
    preference_pairs,  # (prompt, chosen, rejected)
    ranking_loss      # Bradley-Terry model
)

# 3. Reinforcement Learning (PPO)
model_rlhf = train_ppo(
    model_sft,
    reward_model,
    kl_divergence_penalty  # 元モデルからの逸脱制御
)
```

**数学的定式化**：
```python
# PPOの目的関数
L_PPO = E[min(r_θ(a|s)A, clip(r_θ(a|s), 1-ε, 1+ε)A)] - β·KL(π_θ||π_ref)

# Where:
# r_θ: 政策比率 π_θ(a|s)/π_ref(a|s)  
# A: アドバンテージ（報酬モデルからの信号）
# β: KL制約項の重み
```

### **DPO（Direct Preference Optimization）**

**革新的アプローチ**：
```python
# DPOの直接最適化
# Bradley-Terry modelの逆変換を活用
L_DPO = -E[log σ(β(log π_θ(y_w|x) - log π_θ(y_l|x) - 
                  log π_ref(y_w|x) + log π_ref(y_l|x)))]

# Where:
# y_w: 選好された応答 (chosen)
# y_l: 拒否された応答 (rejected)  
# β: 温度パラメータ
# σ: シグモイド関数
```

**実装の簡素化**：
```python
# DPOの実装例
def dpo_loss(model_logprobs, ref_logprobs, chosen_logprobs, 
             rejected_logprobs, beta=0.1):
    # ログ確率の差分計算
    chosen_rewards = beta * (chosen_logprobs - ref_logprobs)
    rejected_rewards = beta * (rejected_logprobs - ref_logprobs)
    
    # 選好損失
    loss = -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards))
    return loss.mean()
```

### **2024-2025年の技術進歩**

**RLHF系列の進化**：
- **PPO-max**: 効率的なPPO実装
- **GRPO (Group Relative Policy Optimization)**: プログラム可能報酬関数
- **WPO (Weighted Preference Optimization)**: オフポリシー選好の重み付け

**DPO系列の発展**：
- **IPO (Identity Preference Optimization)**: 安定性向上
- **KTO (Kahneman-Tversky Optimization)**: 行動経済学理論適用
- **MinorDPO**: 小データでの効率最適化

### **技術的比較分析**

| **観点** | **RLHF** | **DPO** |
|---------|---------|---------|
| **訓練段階** | 3段階（SFT + RM + PPO） | 1段階（直接最適化） |
| **必要モデル数** | 3個（政策・報酬・参照） | 2個（政策・参照） |
| **計算コスト** | 高（強化学習ループ） | 低（教師あり学習） |
| **安定性** | 不安定（RL特有の問題） | 比較的安定 |
| **性能上限** | 高（細かい調整可能） | 中-高（制約あり） |
| **実装難易度** | 高 | 低 |

### **実用的考慮事項**

**RLHFが有利な場面**：
- **複雑なタスク**: 多段階推論、創造的生成
- **細かい制御**: 安全性、バイアス除去
- **大規模運用**: 十分な計算リソースがある場合

**DPOが有利な場面**：
- **リソース制約**: GPU/メモリが限定的
- **高速プロトタイピング**: 迅速な実験サイクル
- **決定論的タスク**: コード生成、構造化出力

### **最新の実装戦略**

**ハイブリッドアプローチ**：
```python
# 段階的最適化戦略
# Stage 1: DPOで基礎アライメント
model_dpo = train_dpo(model_sft, preference_data)

# Stage 2: 特定タスクでRLHF微調整  
model_final = train_rlhf(
    model_dpo, 
    task_specific_rewards,
    limited_iterations=100  # 計算コスト制御
)
```

**モジュラー設計**：
- **SFT**: 基礎能力確立
- **DPO**: 一般的なアライメント
- **RLHF**: タスク固有の精密調整

### **各選択肢の解説**

**A. 学習タイプの混同**: RLHFもDPOも教師あり学習と強化学習の要素を含みます。DPOは「直接的な教師あり学習」でRLから inspiration を得た手法です。

**B. アーキテクチャの違い**: ✅ **正解** - 最も重要な技術的違いです。RLHFは**3段階のパイプライン**（SFT→報酬モデル→PPO）を必要とし、DPOは**選好データから直接最適化**する単段階手法です。

**C. 量子化の関係**: 量子化は両手法とも使用可能で、本質的な違いではありません。QLoRAなどはRLHF/DPO両方で活用されています。

**D. タスク特化性**: 両手法とも汎用的で、生成・分類を問わず様々なタスクに適用可能です。

### **産業界での採用状況**

**RLHF採用例**：
- **ChatGPT**: OpenAIの代表的RLHF実装
- **Claude**: Anthropicの憲法的AI
- **Gemini**: Googleの大規模展開

**DPO採用例**：
- **Llama-3**: Metaの効率的アライメント
- **Mistral**: 高速プロトタイピング
- **Zephyr**: HuggingFaceのオープンソース展開

現代のLLM開発では、タスクの特性とリソース制約に応じて、これらの手法を戦略的に選択・組み合わせることが重要になっています。 