## 問題
### トランスフォーマー
Transformer における Multi-Head Attention の役割として最も適切なものはどれか。

A. 複数の注意ヘッドにより異なる表現部分空間で並列に注意パターンを学習

B. 単一の注意ヘッドでシーケンス全体の平均値を計算

C. 画像の畳み込み処理による特徴抽出

D. 時系列データの未来予測のための回帰分析

## 正解A

解説：
Multi-Head Attention は、選択肢Aの通り、複数の注意ヘッドを用いて異なる表現部分空間で並列に注意パターンを学習する仕組みです。各ヘッドは独立したQuery（Q）、Key（K）、Value（V）の重み行列を持ち、入力の異なる側面や関係性を捕捉できます。例えば、機械翻訳では、一つのヘッドが構文的関係を、別のヘッドが意味的関係を学習する可能性があります。計算式は Attention(Q,K,V) = softmax(QK^T/√d_k)V で表され、各ヘッドの出力を結合（concatenate）して線形変換を適用します。これにより、単一の注意機構では捉えきれない複雑な言語パターンを効果的に学習できます。選択肢Bの平均値計算、選択肢Cの畳み込み処理、選択肢Dの回帰分析は、Multi-Head Attention の機能ではありません。この並列注意処理により、Transformerは豊富な文脈表現を獲得します。 