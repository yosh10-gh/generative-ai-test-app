## 問題
### トランスフォーマー
Transformer アーキテクチャの構成要素として正しいものを全て選びなさい。

A. 位置エンコーディング（Positional Encoding）

B. 畳み込み層（Convolutional Layer）

C. 残差接続（Residual Connection）

D. 長短期記憶ユニット（LSTM Unit）

## 正解A、C

解説：
選択肢A：位置エンコーディング（Positional Encoding）は正解です。Transformerには再帰構造がないため、トークンの位置情報を明示的に与える必要があります。sin/cos関数を用いた固定的な位置エンコーディングまたは学習可能な位置埋め込みにより、シーケンス内の位置関係を表現します。これにより、「猫が魚を食べる」と「魚が猫を食べる」のような語順の違いを認識できます。

選択肢B：畳み込み層（Convolutional Layer）は誤りです。Transformerの基本構成には畳み込み層は含まれません。代わりに、Multi-Head AttentionとFeed-Forward Networkが主要な構成要素です。畳み込み層は主にCNNで使用される技術です。

選択肢C：残差接続（Residual Connection）は正解です。各サブレイヤー（Multi-Head AttentionやFeed-Forward Network）の前後で残差接続が適用され、入力と出力を加算します。これにより勾配の流れが改善され、深いネットワークでも安定した学習が可能になります。Layer Normalizationと組み合わせて使用されます。

選択肢D：長短期記憶ユニット（LSTM Unit）は誤りです。TransformerはRNNやLSTMを使用せず、注意機構のみで構成されています。「Attention is All You Need」というタイトルが示すように、再帰構造を完全に排除したアーキテクチャです。 